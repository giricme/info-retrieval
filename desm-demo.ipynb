{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Search or information retrieval, to use it's more general technical term, is something all of use almost all the time. In any tech stack i have built / worked in in the past decade or so, search has played some role. Typically we use Lucene / Elasticsearch or one of it's technical clones or variants. About a week or so ago i came across some papers on **Neural Information Retrieval**. It led me on some interesting fishing expeditions last couple of weekends and the output of these is, a toy project, that compares classical IR techniques with neural IR.\n",
    "\n",
    "## Classical IR\n",
    "\n",
    "Some background about classical Information Retrieval (IR). Of course there are whole textbooks on this field, but in case you are not fully familiar this section will cover some of the basics so that it is easier to understand this toy project and it's results. Search problem can be simply stated as:\n",
    "\n",
    "**Given a corpus of documents, and a query, the problem of search can be simply stated as finding the best match(es).**\n",
    "\n",
    "Naively one can simply say, if i am looking for *Chennai* simply grep and return all documents in the corpus that contains the word *Chennai*. Of course, needless to say this would be a very naive thing to do. There are whole conferences and disserations dedicated to what \"best match\" means, how to define it, how to quantify it, how to calculate it efficiently at scale etc. So, how does a search system return results ? By scoring each document in a corpus documents for a given input query. The scoring function f(q,d) is a real valued function that takes into account all query terms and all document terms and outputs a score. Scores are then ranked across the corpus and best matches are identified. \n",
    "\n",
    "One of the popular scoring function is Okapi BM25 (BM stands for Best Matching) or BM25 for short. Okapi BM25 is based on the probabilistic retrieval framework. Probabilistic Relevance Model makes an estimation of the probability of finding if a document dj is relevant to a query q. Even though BM25 is motivated by a probabilistic interpretation, I find it easier to understand BM25 as a bag-of-words retrieval function that ranks a set of documents based on the query terms appearing in each document, regardless of the inter-relationship between the query terms within a document (e.g., their relative proximity). \n",
    "\n",
    "\\begin{equation*}\n",
    "Score(D,Q) = \\sum_{i=1}^n IDF(q_i) \\frac {(f(q_i,D)(k_1 + 1))} {(f(q_i,D) + k_1) (1 - b + b \\frac {|D|} {avgdl})}\n",
    "\\end{equation*}\n",
    "\n",
    "where f(q_i,D) is q_i's term frequency in the document D, |D| is the length of the document D in words, and avgdl is the average document length in the text collection from which documents are drawn, k_1 and b are free parameters.\n",
    "\n",
    "\\begin{equation*}\n",
    "IDF(q_i) = \\log \\frac {N - n(q_i) + 0.5} {n(q_i) + 0.5}\n",
    "\\end{equation*}\n",
    "\n",
    "where N is the total number of documents in the collection, and n(q_i) is the number of documents containing q_i.\n",
    "\n",
    "So how are query terms and documents represented ? Vector space model or term vector model is an algebraic model for representing text documents and queries as vectors of identifiers. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. There are several different ways of computing these values, one of the best known schemes is tf-idf weighting. Here tf–idf, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf-idf value increases proportionally to the number of times a word appears in the document, but is often offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. TF stands for Term Frequency and IDF stands for Inverse Document Frequency. For example, if am searching for *Chennai*, a document containing the word *Chennai* many times must score higher that a document that contains the word fewer times. However it is also true that number of occurences should also have some sort of diminishing marginal utility. Presence of second occurence of the word *Chennai* in the document has more importance than say 100th occurence of the same word in the document. IDF is used to offset effect of words that occur too frequently in any language (\"the\" \"a\" etc.) IDF attempts to capture the idea that the specificity of a term can be quantified as an inverse function of the number of documents in which it occurs. So IDF will value rare terms higher than commonly occuring terms.\n",
    "\n",
    "## Neural IR\n",
    "\n",
    "Traditional IR models use local representations of terms for query-document matching. However as most of us can immediately guess, inspecting non-query terms in the document for garnering evidence of relevance can be useful in many cases. For example if a model can learn that *Madras* was the previous name for the same city now called *Chennai* it can be much more efficient in search in some cases. So looking at other terms in the document to infer that this document is about the same city is not possible in traditional term counting based IR approaches.\n",
    "\n",
    "An embedding is a representation of items in a new space such that the properties of, and the relationships between, the items are preserved. The goal of an embedding is to generate a simpler representation. Neural term embedding models are typically trained by setting up a prediction task. Both the term and the features have one-hot representations in the input and the output layers, respectively, and the model learns dense low-dimensional representations in the process of minimizing the prediction error.\n",
    "\n",
    "Word2vec is one such popular embedding. For word2vec, the features for a term are made up of its neighbours within a fixed size window over the text from the training corpus. The skip-gram architecture is a simple one hidden layer neural network. The model has two different weight matrices Win and Wout that are learnable parameters of the models. Win gives us the IN embeddings corresponding to all the input terms and Wout corresponding to the OUT embeddings for the output terms. Generally, only Win is used and Wout is discarded after training, but we will see later that our toy project makes use of both the IN and the OUT embeddings.\n",
    "\n",
    "### Dual Embedding Space Model (DESM)\n",
    "\n",
    "Mitra et al. [https://arxiv.org/pdf/1602.01137.pdf] point out that when using word2vec embeddings for IR it is more appropriate to represent the query terms using the IN embeddings and the document terms using the OUT embeddings of the trained model. In this Dual Embedding Space Model (DESM) the word2vec embeddings are trained on search\n",
    "queries, which empirically performs better than training on document body text. Training on short queries, however, makes the inter-term similarity more pronouncedly Typical (where, “Yale” is closer to “Harvard” and “NYU”) when both terms are represented using their IN vectors—better retrieval performance is achieved instead by using the IN-OUT similarity (where, “Yale” is closer to “faculty” and “alumni”) that mirrors more the Topical notions of relatedness\n",
    "\n",
    "\n",
    "# Goal\n",
    "\n",
    "* This toy project is basically a comparison of BM25 and DESM on a small corpus.\n",
    "* We use input data from https://www.kaggle.com/snapcrack/all-the-news. The data primarily falls between the years of 2016 and July 2017, although there is a not-insignificant number of articles from 2015, and a possibly insignificant number from before then. It is all news articles, a total of **142573** short articles from New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. Sampling isn't quite scientific. It is like a random noisy corpus.\n",
    "* Each row of CSV contains:\n",
    " * an id for the Sqlite database\n",
    " * author name\n",
    " * full date\n",
    " * month\n",
    " * year\n",
    " * title\n",
    " * publication name\n",
    " * article url (not available for all articles)\n",
    " * full article content\n",
    "\n",
    "\n",
    "## Tools\n",
    "\n",
    "* We use **gensim** an open source python library for all our analysis.\n",
    "\n",
    "## Data Prep\n",
    "\n",
    "* **Download the data files from kaggle and store them in \"inputs\" folder** (github does not allow mega files)\n",
    "* We extract just the article from csvs (just the *content* column).\n",
    "* We lower case and use NLTK sentence and word tokenizers.\n",
    "* Once lower cased, it has 519186 unique words. \n",
    "* We name the documents as 1.txt, 2.txt etc. Given how gensim corpora handles document identification this becomes easier to map results back.\n",
    "\n",
    "## BM25 Setup\n",
    "\n",
    "* We first construct a dictionary and corpus using **gensim**.\n",
    "* We then use it's BM25 scoring functions.\n",
    "* Note that documents are identified by the order in which they are added to the corpora. So numbered input files come in handy.\n",
    "\n",
    "## DESM Setup\n",
    "\n",
    "* We use **gensim** word2vec implementation to generate word2vec embeddings.\n",
    "* Word2Vec training on 592108745 raw words (444871464 effective words) took 5432.0s, 81898 effective words/s on my macbook pro.\n",
    "* Embeddings produces are 100 dimensional.\n",
    "* In the word2vec model, there are two linear transforms that take a word in vocab space to a hidden layer (the \"in\" vector), and then back to the vocab space (the \"out\" vector). Usually this out vector is discarded after training.  However when we do `model.save` using **gensim** it saves both input and output weights. The input and output embeddings are syn0 and syn1 (or syn1neg, for negative sampling), respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os, csv, glob, json, uuid, pickle, math\n",
    "import nltk \n",
    "import gensim, logging\n",
    "import numpy as np, scipy, pandas as pd\n",
    "from operator import itemgetter\n",
    "from IPython.display import HTML, display\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTENT_INDEX = 9\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "CONTENT_PATH = './inputs/contents/'\n",
    "TOKENS_PATH = './inputs/tokens/'\n",
    "CENTROIDS_PATH = './inputs/centroids/'\n",
    "BM25_PATH = './inputs/bm25/'\n",
    "\n",
    "if not os.path.exists(CONTENT_PATH):\n",
    "    os.makedirs(CONTENT_PATH)\n",
    "    \n",
    "if not os.path.exists(TOKENS_PATH):\n",
    "    os.makedirs(TOKENS_PATH)\n",
    "    \n",
    "if not os.path.exists(CENTROIDS_PATH):\n",
    "    os.makedirs(CENTROIDS_PATH)\n",
    "\n",
    "if not os.path.exists(BM25_PATH):\n",
    "    os.makedirs(BM25_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prep stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for fname in glob.iglob('./inputs/*.csv', recursive=False):\n",
    "    f = open(fname)\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        count = count + 1\n",
    "        content = line[CONTENT_INDEX]\n",
    "        cname = CONTENT_PATH + str(count) + '.txt'\n",
    "        tname = TOKENS_PATH + str(count) + '.tokens'\n",
    "        cf = open(cname, 'w')\n",
    "        cf.write(content)\n",
    "        cf.close()\n",
    "        tf = open(tname, 'w')\n",
    "        for sentence in nltk.sent_tokenize(content):\n",
    "            tf.write(\"%s\\n\" % sentence.lower())\n",
    "        tf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want do not want to load all documents into memory. Even for this small corpus it is over 140K documents. We create a lazy python iterator that word2vec model uses to grab data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in glob.iglob(self.dirname +'*.tokens', recursive=True):\n",
    "            for line in open(fname):\n",
    "                yield nltk.word_tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "sentences = MySentences('./inputs/tokens/') \n",
    "model1 = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model. In the word2vec model, there are two linear transforms that take a word in vocab space to a hidden layer (the \"in\" vector), and then back to the vocab space (the \"out\" vector). Usually this out vector is discarded after training.  However when we do `model.save` using **gensim** it saves both input and output weights. The input and output embeddings are syn0 and syn1 (or syn1neg, for negative sampling), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.save('./model/w2v-lc.model')\n",
    "model1.wv.save_word2vec_format('./model/w2v-lc.model.bin', binary=True)\n",
    "vocab = dict([(k, v.index) for k, v in model1.wv.vocab.items()])\n",
    "with open('./model/w2v-lc-vocab.json', 'w') as f:\n",
    "    f.write(json.dumps(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a simple test. It is interesting that it thinks texas : senate = alabama : congress. Not a bad start to have learnt some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.wv.most_similar(positive=['texas', 'senate'], negative=['alabama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for fname in glob.iglob('./inputs/contents/*.txt', recursive=False):\n",
    "    for line in open(fname):\n",
    "        centroid_in = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(line.lower())]), axis=0))\n",
    "        centroid_out = (np.mean(np.array([get_embedding(x, out=True) for x in nltk.word_tokenize(line.lower())]), axis=0))\n",
    "        out_dict = { fname : (centroid_in, centroid_out) }\n",
    "        pickle_file = './inputs/centroids/' + os.path.basename(fname).replace('.txt', '.p')\n",
    "        pickle.dump(out_dict, open(pickle_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get on to BM25. First we build the dictionary, followed by corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BM25Sentences(object):\n",
    "    def __init__(self, pattern):\n",
    "        self.pattern = pattern\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in glob.iglob(self.pattern, recursive=True):\n",
    "            for line in open(fname):\n",
    "                yield nltk.word_tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = BM25Sentences('./inputs/tokens/*.tokens')\n",
    "dictionary = gensim.corpora.Dictionary(line for line in sentences)\n",
    "dictionary.compactify()\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.save('./inputs/bm25/allnews.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bm25dict = dictionary.load('./inputs/bm25/allnews.dict') \n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        self.count = 142573\n",
    " \n",
    "    def __iter__(self):\n",
    "        for x in range(self.count):\n",
    "            fname = self.dirname + str(x+1) + '.tokens'\n",
    "            doc = open(fname).read().replace('\\n', '')\n",
    "            yield bm25dict.doc2bow(nltk.word_tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "citer = MyCorpus(TOKENS_PATH)\n",
    "corpus = [x for x in citer]\n",
    "print (len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gensim.corpora.MmCorpus.serialize('./inputs/bm25/allnewscorpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bm25corpus = gensim.corpora.MmCorpus('./inputs/bm25/allnewscorpus.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "We test both DESM and BM25 with the same input query. The query we choose is kinda long (by query standards) and is rather generic. It is **political stability and economic health**. We stick to top 5 results.\n",
    "\n",
    "## DESM Test\n",
    "\n",
    "* As mentioned in the DESM paper, we compute the centroid of documents and compute the cosine similarity of the query words with document centroid.\n",
    "* We present the results with both DESM-IN-OUT and DESM-IN-IN metrics (see paper for details)\n",
    "\n",
    "## BM25 Test\n",
    "\n",
    "* We compute BM25 scores across the corpus for the input query and pick the top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('./model/w2v-lc.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centroid_dict = {}\n",
    "for fname in glob.iglob('./inputs/centroids/*.p', recursive=False):\n",
    "    centroid_dict.update(pickle.load(open(fname, \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_centroid_dict = {k: centroid_dict[k] for k in centroid_dict if not np.isnan(centroid_dict[k][0]).any()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(x, out=False):\n",
    "    if x in model.wv.vocab:\n",
    "        if out == True:\n",
    "            return model.syn1neg[model.wv.vocab[x].index]\n",
    "        else:\n",
    "            return model[x]\n",
    "    else:\n",
    "        return np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_document(q_embeddings, d_centroid):\n",
    "    individual_csims = [(1 - scipy.spatial.distance.cosine(qin, d_centroid)) for qin in q_embeddings]\n",
    "    return (sum(individual_csims)/len(q_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bm25dict = gensim.corpora.Dictionary().load('./inputs/bm25/allnews.dict') \n",
    "bm25corpus = gensim.corpora.MmCorpus('./inputs/bm25/allnewscorpus.mm')\n",
    "bm25 = gensim.summarization.bm25.BM25(bm25corpus)\n",
    "average_idf = sum(float(val) for val in bm25.idf.values()) / len(bm25.idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = 'political stability and economic health'\n",
    "query_words = nltk.word_tokenize(query.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = bm25.get_scores(bm25dict.doc2bow(query_words), average_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_result = ['./inputs/contents/'+str(x+1)+'.txt' for x in (sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:5])]\n",
    "for fname in best_result:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_ins = [get_embedding(x) for x in query_words]\n",
    "q_len = len(query_ins)\n",
    "print('Num words in query: ', len(query_words), 'Num query word in vectors: ', q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_in_in = []\n",
    "scores_in_out = []\n",
    "for k,v in clean_centroid_dict.items():\n",
    "    scores_in_in.append((k, score_document(query_ins, v[0])))\n",
    "    scores_in_out.append((k, score_document(query_ins, v[1])))\n",
    "\n",
    "scores_in_in = sorted(scores_in_in, key=itemgetter(1), reverse=True)\n",
    "scores_in_out = sorted(scores_in_out, key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('TOP 5 IN-IN:')\n",
    "top_5_in_in = [x[0] for x in scores_in_in[:5]]\n",
    "\n",
    "for fname in top_5_in_in:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('TOP 5 IN-OUT:')\n",
    "top_5_in_out = [x[0] for x in scores_in_out[:5]]\n",
    "\n",
    "for fname in top_5_in_out:\n",
    "    print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We present the **Top 5** document ids for three modes, BM25, DESM-IN-IN, DESM-IN-OUT. You can find the contents of these documents in this google sheet. https://docs.google.com/spreadsheets/d/1nbv8yICv8p1eIlhp9H5LQjKoSADoUrP8QrjSZkOeNfc/edit#gid=1769371456\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>BM25       </td><td style=\"text-align: right;\"> 30931</td><td style=\"text-align: right;\"> 40023</td><td style=\"text-align: right;\">71852</td><td style=\"text-align: right;\">133532</td><td style=\"text-align: right;\">  1620</td></tr>\n",
       "<tr><td>DESM-IN-IN </td><td style=\"text-align: right;\">140797</td><td style=\"text-align: right;\"> 32221</td><td style=\"text-align: right;\">31472</td><td style=\"text-align: right;\"> 39594</td><td style=\"text-align: right;\">135444</td></tr>\n",
       "<tr><td>DESM-IN-OUT</td><td style=\"text-align: right;\"> 73280</td><td style=\"text-align: right;\">140797</td><td style=\"text-align: right;\">32221</td><td style=\"text-align: right;\"> 42404</td><td style=\"text-align: right;\"> 42105</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = [[\"BM25\",30931, 40023, 71852, 133532, 1620],\n",
    "         [\"DESM-IN-IN\", 140797, 32221, 31472, 39594, 135444],\n",
    "         [\"DESM-IN-OUT\", 73280, 140797, 32221, 42404, 42105]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The documents returned by DESM are clearly much higher quality. They certainly are documents related to **political stability and economic health**. However documents returned by BM25 are solely based on term matchings.\n",
    "\n",
    "It is quite impressive how word2vec trained on the corpus has learnt word contexts. I was very skeptical about taking the centroid of w2v representations (as mentioned in DESM paper) (as someone who is used to dealing with loads of geo spatial data where i routinely hit centroid outside of the object in question, but i digress) i was wondering how taking the centroid would capture all that the doc has to say. It works remarkably well. Also to some extent all my content is news articles. Typically they tend to be focussed on a single topic, instead of meandering prose.\n",
    "\n",
    "One could also argue fairly that my query is biased. It is sort of vague and is not specific like looking for **Mango**. Obviously word2vec is designed to excel at it as compared to BM25, which is based on just bag of words with no contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Future Explorations\n",
    "\n",
    "* As DESM paper mentions not all embeddings are created equal. It might be fun to see how Glove does in place of Word2Vec.\n",
    "* Convolution Neural Nets excel at image classifications. How about using CNNs to recognize contextual passages in a document ? There are some interesting papers in this area that might be fun to dig into.\n",
    "* Lots more to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
